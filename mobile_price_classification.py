# -*- coding: utf-8 -*-
"""Mobile Price Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16d5pX6u6RsmzWBPOPKX5m8G5gcTtTdOK
"""

import numpy as np
import pandas as pd

data=pd.read_csv("/content/drive/MyDrive/ML Mentorness/train (1).csv")

data.head()

data.info()

data.columns

print(data.isnull().sum())

new_data.describe()

data.corr()

import seaborn as sns
sns.heatmap(data.corr(),cmap='Blues',annot=True)

data.hist(figsize=(10,10))

sns.boxplot(data['battery_power'])

sns.boxplot(data['blue'])

sns.boxplot(data['clock_speed'])

sns.boxplot(data['dual_sim'])

sns.boxplot(data['fc'])

sns.boxplot(data['four_g'])

sns.boxplot(data['int_memory'])

sns.boxplot(data['m_dep'])

sns.boxplot(data['mobile_wt'])

sns.boxplot(data['pc'])

sns.boxplot(data['px_height'])

sns.boxplot(data['px_width'])

sns.boxplot(data['ram'])

sns.boxplot(data['sc_h'])

sns.boxplot(data['sc_w'])

sns.boxplot(data['talk_time'])

sns.boxplot(data['three_g'])

sns.boxplot(data['touch_screen'])

sns.boxplot(data['wifi'])

"""# Some outliers are there in Fast charging, Px height, Three G values."""

from sklearn.ensemble import IsolationForest
# Extract the 'px_height' column for outlier detection
fc_data = data['fc'].values.reshape(-1, 1)

# Create an Isolation Forest model
isolation_forest = IsolationForest(contamination='auto', random_state=42)

# Fit the model and predict outliers
isolation_forest.fit(fc_data)
outliers = isolation_forest.predict(fc_data)

# Extract indices of outliers
outlier_indices = (outliers == -1)

# Print the indices of the outliers
print("Indices of outliers:", data[outlier_indices].index.tolist())

# Remove rows containing outliers
fc_data_filtered = data[~outlier_indices]

# Reset the index of the DataFrame
fc_data_filtered.reset_index(drop=True, inplace=True)

# Extract the 'px_height' column for outlier detection
px_height_data = fc_data_filtered['px_height'].values.reshape(-1, 1)

# Create an Isolation Forest model
isolation_forest = IsolationForest(contamination='auto', random_state=42)

# Fit the model and predict outliers
isolation_forest.fit(px_height_data)
outliers = isolation_forest.predict(px_height_data)

# Extract indices of outliers
outlier_indices = (outliers == -1)

# Print the indices of the outliers
print("Indices of outliers:", fc_data_filtered[outlier_indices].index.tolist())

# Remove rows containing outliers
px_height_data_filtered = fc_data_filtered[~outlier_indices]

# Reset the index of the DataFrame
px_height_data_filtered.reset_index(drop=True, inplace=True)

# Extract the 'px_height' column for outlier detection
three_g_data = px_height_data_filtered['three_g'].values.reshape(-1, 1)

# Create an Isolation Forest model
isolation_forest = IsolationForest(contamination='auto', random_state=42)

# Fit the model and predict outliers
isolation_forest.fit(three_g_data)
outliers = isolation_forest.predict(three_g_data)

# Extract indices of outliers
outlier_indices = (outliers == -1)

# Print the indices of the outliers
print("Indices of outliers:", px_height_data_filtered[outlier_indices].index.tolist())

# Remove rows containing outliers
three_g_data_filtered = px_height_data_filtered[~outlier_indices]

# Reset the index of the DataFrame
three_g_data_filtered.reset_index(drop=True, inplace=True)

new_data=three_g_data_filtered

sns.boxplot(new_data['fc'])

sns.boxplot(new_data['px_height'])

sns.boxplot(new_data['three_g'])

new_data

new_data['price_range'].value_counts()

from imblearn.under_sampling import RandomUnderSampler
from collections import Counter

X = new_data.drop("price_range", axis=1)
Y = new_data["price_range"]

# Instantiate RandomUnderSampler
rus = RandomUnderSampler(random_state=0)

# Resample the dataset
x, y = rus.fit_resample(X, Y)

# Check class distribution after resampling
print("Class distribution after resampling:", Counter(y))

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x, y,test_size=0.3,random_state=0)

x_train.head()

y_train.head()

from sklearn.feature_selection import mutual_info_classif #determine the mutual information
mutual_info=mutual_info_classif(x_train,y_train)
mutual_info

mutual_info=pd.Series(mutual_info)
mutual_info.index=x_train.columns
mutual_info.sort_values(ascending=False)

mutual_info.sort_values(ascending=False).plot.bar(figsize=(20,8)) # plot the ordered mutual info

"""# Random access memory(ram) has highest mutual information value."""

from sklearn.feature_selection import SelectKBest
top_ten_col=SelectKBest(mutual_info_classif, k=10)
top_ten_col.fit(x_train,y_train)
x_train.columns[top_ten_col.get_support()]

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Initialize classifiers
classifiers = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier(),
    "Logistic Regression": LogisticRegression()
}

# Train and evaluate classifiers
results = {}
for name, clf in classifiers.items():
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)*100
    precision = precision_score(y_test, y_pred, average='weighted')*100
    recall = recall_score(y_test, y_pred, average='weighted')*100
    f1 = f1_score(y_test, y_pred, average='weighted')*100
    cm = confusion_matrix(y_test, y_pred)
    results[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-score': f1, 'Confusion Matrix': cm}

# Print results
for name, result in results.items():
    print(f"Classifier: {name}")
    print(f"Accuracy: {result['Accuracy']:.2f}%")
    print(f"Precision: {result['Precision']:.2f}%")
    print(f"Recall: {result['Recall']:.2f}%")
    print(f"F1-score: {result['F1-score']:.2f}%")
    print(f"Confusion Matrix:\n{result['Confusion Matrix']}")
    print("---------------------------------------------")

"""# Support Vector Classifier(svc) has more accuracy, precision compared to other classifers."""

svm=SVC()

param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2']
}

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 10, 100],
    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']
}

# Create SVC classifier
svm = SVC()

# Perform GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(x_train, y_train)

# Get the best parameters and the best estimator
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_

svm=SVC()
svm.fit(x_train, y_train)

data_test = pd.read_csv('/content/drive/MyDrive/ML Mentorness/test.csv')
data_test

data_test.columns

data_test.drop('id',axis=1,inplace=True)
x_test = data_test
x_test

"""# Predicting Price range of the test data"""

y_pred = svm.predict(x_test)
y_pred